{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBXZFro_Se_A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RiddleDataset(Dataset):\n",
        "    def __init__(self, csv_file, tokenizer, max_length=128):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.data['text'] = self.data.apply(lambda row: f\"Riddle: {row['Riddle']}\", axis=1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data.iloc[idx]['text']\n",
        "        encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, padding='max_length', return_tensors=\"pt\")\n",
        "        input_ids = encoding.input_ids.squeeze()\n",
        "        attention_mask = encoding.attention_mask.squeeze()\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": input_ids}"
      ],
      "metadata": {
        "id": "ZYWaQY2XSmgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    model_name = \"TinyLlama/TinyLlama_v1.1\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    dataset = RiddleDataset(\"riddlefactory.csv\", tokenizer)\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    # training_args = TrainingArguments(\n",
        "    #     output_dir=\"./results\",\n",
        "    #     num_train_epochs=5,\n",
        "    #     per_device_train_batch_size=4,\n",
        "    #     per_device_eval_batch_size=4,\n",
        "    #     evaluation_strategy=\"epoch\",\n",
        "    #     logging_steps=1,\n",
        "    #     save_strategy=\"epoch\",\n",
        "    #     weight_decay=0.01,\n",
        "    #     learning_rate=5e-5,\n",
        "    #     optim=\"adamw_torch\",\n",
        "    #     fp16=True if torch.cuda.is_available() else False,\n",
        "    #     load_best_model_at_end=True,\n",
        "    #     metric_for_best_model=\"loss\",\n",
        "    #     greater_is_better=False,\n",
        "    #     seed=42,\n",
        "    # )\n",
        "    training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_steps=1,\n",
        "    save_strategy=\"epoch\",\n",
        "    weight_decay=0.1,\n",
        "    learning_rate=3e-5,\n",
        "    warmup_steps=50,\n",
        "    fp16=True if torch.cuda.is_available() else False,\n",
        "    load_best_model_at_end=True,\n",
        "    gradient_accumulation_steps=2,\n",
        "    max_grad_norm=1.0,\n",
        "    seed=42,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    model.save_pretrained(\"./fine_tuned_tinyllama\")\n",
        "    tokenizer.save_pretrained(\"./fine_tuned_tinyllama\")\n",
        "\n"
      ],
      "metadata": {
        "id": "liQCl10WSqrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "qRmXhN6jTjiI",
        "outputId": "bac63d17-129e-458e-9f5a-d29b16e83c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbuzzgrewal\u001b[0m (\u001b[33mbuzzgrewal-fast\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250306_170735-6alv38iq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/buzzgrewal-fast/huggingface/runs/6alv38iq' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/buzzgrewal-fast/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/buzzgrewal-fast/huggingface' target=\"_blank\">https://wandb.ai/buzzgrewal-fast/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/buzzgrewal-fast/huggingface/runs/6alv38iq' target=\"_blank\">https://wandb.ai/buzzgrewal-fast/huggingface/runs/6alv38iq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 09:51, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>11.154200</td>\n",
              "      <td>11.143553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>11.188400</td>\n",
              "      <td>11.143553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>11.272300</td>\n",
              "      <td>11.143553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>11.168700</td>\n",
              "      <td>11.143553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>10.971700</td>\n",
              "      <td>11.143553</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
      ],
      "metadata": {
        "id": "X_zeIPcFVShm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model_on_riddle_prompts(model_path=\"./fine_tuned_tinyllama\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "    model.eval()\n",
        "\n",
        "    prompts = [\n",
        "        \"Riddle:\",\n",
        "        \"Generate a math riddle:\",\n",
        "        \"Math Puzzle:\",\n",
        "        \"Write a new math riddle:\",\n",
        "        \"Create a math riddle with its answer:\"\n",
        "    ]\n",
        "\n",
        "    for i, prompt in enumerate(prompts):\n",
        "        encoding = tokenizer(prompt, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=50)\n",
        "        input_ids = encoding.input_ids\n",
        "        attention_mask = encoding.attention_mask\n",
        "\n",
        "        output_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=50,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        print(f\"Prompt {i+1}:\\n{prompt}\")\n",
        "        print(f\"Generated Riddle:\\n{generated_text}\\n{'-'*50}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_model_on_riddle_prompts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3_viddVVVrP",
        "outputId": "ca45e070-28fa-444a-ac82-5545b45ca5ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt 1:\n",
            "Riddle:\n",
            "Generated Riddle:\n",
            "Riddle: F  Y   I   I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I \n",
            "--------------------------------------------------\n",
            "Prompt 2:\n",
            "Generate a math riddle:\n",
            "Generated Riddle:\n",
            "Generate a math riddle: R Dll.\"\" R.\"\".\" R.\".\" \".\" R.\".\"\" R \" .\"  \" r.\" .\" r\".\" r\".\" r\" R R \" R R.\" R \" R R\".\" R r\n",
            "--------------------------------------------------\n",
            "Prompt 3:\n",
            "Math Puzzle:\n",
            "Generated Riddle:\n",
            "Math Puzzle: 3.132.313.22 00:12:31:16 2009-12-03\n",
            "00017820 20:1\n",
            "--------------------------------------------------\n",
            "Prompt 4:\n",
            "Write a new math riddle:\n",
            "Generated Riddle:\n",
            "Write a new math riddle: 2nd writing 2nd going writing 3 nd writing 2nd going writing 2ndgoing writing 2nd reading 2nd going 3nd nd 2nd going nd 2ndgoing nd \n",
            "--------------------------------------------------\n",
            "Prompt 5:\n",
            "Create a math riddle with its answer:\n",
            "Generated Riddle:\n",
            "Create a math riddle with its answer: The young m being.\n",
            "The book itself is well done, I did find the cover to be very odd to me, but the author is not a big fan of the cover so I guess it's not as odd to him.\n",
            "I\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "def test_model(model_path=\"./fine_tuned_tinyllama\", prompt=\"Riddle:\", num_riddles=5):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "    model.eval()\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=50,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        num_return_sequences=num_riddles,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "\n",
        "    for i, output in enumerate(outputs):\n",
        "        generated_text = tokenizer.decode(output, skip_special_tokens=True)\n",
        "        print(f\"Generated Riddle {i+1}:\\n{generated_text}\\n{'-'*50}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJY3etHZYDBe",
        "outputId": "85f865ec-e335-49aa-e8d4-c20f84af141b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Riddle 1:\n",
            "Riddle: J Row R R Row RR R R R R R R R RR R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R\n",
            "--------------------------------------------------\n",
            "Generated Riddle 2:\n",
            "Riddle: Rangerle Danger Danger Danger D A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A\n",
            "--------------------------------------------------\n",
            "Generated Riddle 3:\n",
            "Riddle: Ranger.\n",
            "At the first-time meeting of the new 150th Assembly District, which includes all of the Town of Haverstraw, the district's new chairwoman said her goal for the\n",
            "--------------------------------------------------\n",
            "Generated Riddle 4:\n",
            "Riddle:  Iam.\n",
            "Amazon.com : BLUE TAILS FOR BOYS BLUE AND GREY SWEATSHIRT BOYS CLOTHING.\n",
            "--------------------------------------------------\n",
            "Generated Riddle 5:\n",
            "Riddle:\n",
            "All the above information is correct. However, you can contact your doctor if you have any concerns.\n",
            "I've just had a coughing fit. What should I do now?\n",
            "If you've just had\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the dataset is very small and the model is not fine tuned enough to generate any senseable riddles with a Training loss of around 10. So the riddles are not making any sense and even they are not riddles."
      ],
      "metadata": {
        "id": "NBcS7EAwhje0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r Numenigma_tinyLlama.zip ./fine_tuned_tinyllama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n60F4Cm8gCW7",
        "outputId": "c983ff59-dbb7-4a59-f39a-53982712520a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: fine_tuned_tinyllama/ (stored 0%)\n",
            "  adding: fine_tuned_tinyllama/generation_config.json (deflated 29%)\n",
            "  adding: fine_tuned_tinyllama/tokenizer_config.json (deflated 68%)\n",
            "  adding: fine_tuned_tinyllama/tokenizer.json (deflated 85%)\n",
            "  adding: fine_tuned_tinyllama/model.safetensors (deflated 7%)\n",
            "  adding: fine_tuned_tinyllama/config.json (deflated 49%)\n",
            "  adding: fine_tuned_tinyllama/tokenizer.model (deflated 55%)\n",
            "  adding: fine_tuned_tinyllama/special_tokens_map.json (deflated 73%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLink\n",
        "FileLink(r'Numenigma_tinyLlama.zip')"
      ],
      "metadata": {
        "id": "3-EL3iJMj3-I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2f497e70-3cee-407e-88e0-381883df4978"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "/content/Numenigma_tinyLlama.zip"
            ],
            "text/html": [
              "<a href='Numenigma_tinyLlama.zip' target='_blank'>Numenigma_tinyLlama.zip</a><br>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}